{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lardon playground\n",
    "\n",
    "**lardon** is a front-end for dynamic data import of large files, using the numpy.memmap interface to easily index large memory arrays without the entire import of the corresponding file. It is designed to be compatible with every format, using callback functions / environments how to convert given files into numpy arrays. It also provides machine learning-oriented features such as random indexing, data/metadata, and scattering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple parsing / loading\n",
    "\n",
    "Here we will briefly explain how to parse and load data. Regarding parsing, `lardon` propose two different features \n",
    "- the `compile` function, that will list all the files to parse according a valid extension or a valid regexp, and parse it in the destination folder\n",
    "- the `LardonParser` environment, where you can register data and files progressively : whether by generating the files and registring it with the `register` functions, or by iterating in the files detected by the parser.\n",
    "\n",
    "### Using `compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "\n",
    "# first, generate some dumb data\n",
    "data_path = \"tests/dumb_dataset\"\n",
    "original_data_path = \"tests/dumb_dataset/data\"\n",
    "n_examples = 10\n",
    "data_shape = (5,7,13)\n",
    "if not os.path.isdir(data_path):\n",
    "    os.makedirs(data_path)\n",
    "if not os.path.isdir(f\"{original_data_path}\"):\n",
    "    os.makedirs(f\"{original_data_path}\")\n",
    "for n in range(n_examples): \n",
    "    data = np.reshape(np.arange(np.prod(data_shape)), data_shape)\n",
    "    np.save(f\"{original_data_path}/dumb_{n}.npy\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting files...: 100%|██████████| 10/10 [00:00<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7, 13)\n",
      "(5, 7, 13) (1, 7, 13) (1, 2, 13)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from lardon import compile\n",
    "\n",
    "def dumb_callback(filepath: str):\n",
    "    data = np.load(filepath)\n",
    "    metadata = {'label': random.randrange(10)}\n",
    "    # a callback retutns the original data, plus optional metadata as a dictionary.\n",
    "    # if you don't want any metadata, return an empty dictionary with dict()\n",
    "    return data, metadata\n",
    "\n",
    "parsed_path = data_path + \"/parsing\"\n",
    "offline_list = compile(original_data_path, parsed_path, valid_exts = ['.npy'], callback=dumb_callback)\n",
    "\n",
    "# the lardon package provides an `OfflineDataList`, that contains a list of elements called `OfflineEntry`.\n",
    "# OfflineEntry imports the corresponding data when called.\n",
    "offline_entry = offline_list.entries[0]\n",
    "data = offline_entry()\n",
    "print(data.shape)\n",
    "\n",
    "# Indexing the OfflineDataList will dynamically call the OfflineEntry, with the targeted indices such that\n",
    "# only relevant part of the files are loaded using memmap.\n",
    "print(offline_list[0].shape, offline_list[0, 1:2].shape, offline_list[0, 1:2, 3:5].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `LardonParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chemla/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:177: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order='C', ndmin=1)\n"
     ]
    }
   ],
   "source": [
    "from lardon import LardonParser\n",
    "\n",
    "with LardonParser(original_data_path, parsed_path, force=True, valid_exts=[\".npy\"], callback=dumb_callback) as parser:\n",
    "    for f in parser.files:\n",
    "        data = parser.callback(f)\n",
    "        metadata = {'label': random.randrange(10)}\n",
    "        parser.register(data, metadata, filename=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lardon import LardonParser\n",
    "import numpy as np\n",
    "# with LardonParser, you can also parse data generated on-the-fly. In this case,\n",
    "# pass None as original_data_path (parser.files will then be empty.)\n",
    "\n",
    "generated_path = \"tests/generated_dataset\"\n",
    "with LardonParser(None, generated_path, force=True) as parser:\n",
    "    for freq in range(100,1000,100):\n",
    "        data = np.sin( 2 * np.pi * freq * np.linspace(0., 1., 44100))\n",
    "        parser.register(data, {'freq': freq}, filename=f\"sin_{freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7, 13)\n",
      "[\n",
      "\tOfflineEntry(selector: Selector(), file: tests/dumb_dataset/parsing/dumb_3.npy),\n",
      "\tOfflineEntry(selector: Selector(), file: tests/dumb_dataset/parsing/dumb_4.npy),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from lardon import parse_folder\n",
    "\n",
    "offline_data_list = parse_folder(parsed_path)\n",
    "x, y = offline_data_list[0]\n",
    "print(x.shape)\n",
    "\n",
    "# you can also filter the loaded files with the `files` keyword argument.\n",
    "files = [\"dumb_3.npy\", \"dumb_4.npy\"]\n",
    "offline_data_list = parse_folder(parsed_path, files=files)\n",
    "print(offline_data_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data batching\n",
    "\n",
    "`OfflineDataList` can have entries of different size, such that calls like `offline_data_list[:2]` can have ambiguous meanings. The behavior of `OfflineDataList` can be set at loading with the `batch_mode` argument as follows :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without pad: \n",
      "[(1, 2, 9), (3, 2, 9), (3, 3, 9), (2, 3, 9)]\n",
      "with pad : \n",
      "(10, 3, 3, 9)\n",
      "with crop : \n",
      "(10, 1, 1, 9)\n"
     ]
    }
   ],
   "source": [
    "import random, numpy as np\n",
    "from lardon import parse_folder, LardonParser\n",
    "\n",
    "path = \"tests/various_size_dataset\"\n",
    "# generate fake dataset with various sizes\n",
    "\n",
    "with LardonParser(None, path, force=True) as parser:\n",
    "    for i in range(10):\n",
    "        data = np.random.rand(random.randrange(1, 4), random.randrange(1, 4), 9)\n",
    "        parser.register(data, {})\n",
    "\n",
    "offline_list = parse_folder(path)\n",
    "# by default, if data cannot be stacked, offline_list will return a list\n",
    "print(\"without pad: \")\n",
    "print([x.shape for x in offline_list[:4]])\n",
    "\n",
    "# the batch_mode keyword allows to set how the offline list will return its\n",
    "# values in case of unconsistent shape.\n",
    "offline_list = parse_folder(path, batch_mode=\"pad\",\n",
    "                            batch_args={'mode':'constant', 'constant_values':2})\n",
    "batched_import = offline_list[:]\n",
    "print(\"with pad : \")\n",
    "print(batched_import.shape)\n",
    "\n",
    "# batch_mode can also be set to crop, where in this case data shape will be cropped\n",
    "# to the smallest element\n",
    "offline_list = parse_folder(path, batch_mode=\"crop\",\n",
    "                            batch_args={'mode':'constant', 'constant_values':2})\n",
    "batched_import = offline_list[:]\n",
    "print(\"with crop : \")\n",
    "print(batched_import.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random slices and scattering\n",
    "\n",
    "`lardon` also implements ways of randomly picking into files, automatizing boring routines in data handling. Also, `OfflineDataList` can be scattered among a given axis, allowing to \"flatten\" a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44100,)\n",
      "[14706 14707 14708 14709]\n",
      "[22131 22132 22133 22134]\n",
      "[23096 23097 23098 23099]\n",
      "[10269 10270 10271 10272]\n"
     ]
    }
   ],
   "source": [
    "import random, numpy as np\n",
    "from lardon import parse_folder, LardonParser, randslice\n",
    "\n",
    "path = \"tests/long_datasets\"\n",
    "\n",
    "# generate fake dataset with long shapes \n",
    "with LardonParser(None, path, force=True) as parser:\n",
    "    for i in range(2):\n",
    "        data = np.arange(44100)\n",
    "        parser.register(data, {})\n",
    "\n",
    "offline_list = parse_folder(path)\n",
    "for i in range(4):\n",
    "    x = offline_list[0, randslice(4)]\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "import random, numpy as np\n",
    "from lardon import parse_folder, OfflineDataList\n",
    "\n",
    "path = \"tests/dumb_dataset/parsing\"\n",
    "offline_list = parse_folder(path)\n",
    "print(offline_list.shape)\n",
    "offline_entries = offline_list.entries\n",
    "scattered_entries = sum([x.scatter(0) for x in offline_entries], [])\n",
    "scattered_list = OfflineDataList(scattered_entries)\n",
    "print(scattered_list.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3074b235e4e8a5f8a49a19b8af44cc3e33a6956109c9cc3465519429c5c1af66"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
